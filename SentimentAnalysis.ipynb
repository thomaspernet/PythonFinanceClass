{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Create connection with Mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#nltk.download('wordnet')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "client = MongoClient('localhost', 27017)\n",
    "\n",
    "### Client is the database\n",
    "db = client['StockTwitClass101']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Pipeline To create Sentiment\n",
    "\n",
    "Related paper: [Intraday online investor sentiment and return patterns in the U.S.\n",
    "stock market](https://docs.google.com/file/d/1L8bS8vNTXS-HWToP4zMpqfT308n-LxZb/edit)\n",
    "\n",
    "L1/L2 Lexicon: [here](http://www.thomas-renault.com/data.php)\n",
    "\n",
    "A) Create a Function to prepare the data\n",
    "    \n",
    "    1. Keep only twit with sentiment either `Bullish` or `Bearish` and remove multiple stock twits\n",
    "   \n",
    "    2. take negation into account, we add the prefix \"negtag_\" to all words following \"not\",\"no\",\"none\",\"neither\",\"never\" or “nobody”\n",
    "    \n",
    "    3. Convert digit to \"_digit\"\n",
    "    \n",
    "    4. Remove when mention a user\n",
    "    \n",
    "    5. lemmatize corpus\n",
    "    \n",
    "    6. Prepare train/test set\n",
    "    \n",
    "B) Build the Vectorization\n",
    "C) Construct the Naive classifier\n",
    "D) Predict out of sample\n",
    "\n",
    "### note about Lemmatization\n",
    "\n",
    "Lemmatization reduces words to their base word, which is linguistically correct lemmas. It transforms root word with the use of vocabulary and morphological analysis. Lemmatization is usually more sophisticated than stemming. Stemmer works on an individual word without knowledge of the context. For example, The word \"better\" has \"good\" as its lemma. This thing will miss by stemming because it requires a dictionary look-up\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Create a Function to prepare the data\n",
    "\n",
    "Step : 1\n",
    "       \n",
    "       - Exclude multi tickers\n",
    "\n",
    "Step : 2\n",
    "       \n",
    "       - take negation into account:\n",
    "       \n",
    "       - \"not\",\"no\",\"none\",\"neither\",\"never\" or “nobody”\n",
    "\n",
    "Step : 3\n",
    "       \n",
    "       - Convert digit to \"_digit\"\n",
    "\n",
    "Step : 4\n",
    "        \n",
    "       - Remove @USER\n",
    "\n",
    "Step : 5\n",
    "       \n",
    "       - Remove unicode issue\n",
    "        \n",
    "Step 6: Lemmanize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def metatransformation(query, to_train=True):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Query: MongoDB query \n",
    "    to_train:  True: return a train and test dataset\n",
    "    False: return only data to predict out of sample\n",
    "    \n",
    "    Step : 1\n",
    "        - Exclude multi tickers\n",
    "\n",
    "    Step : 2\n",
    "        - take negation into account:\n",
    "        - \"not\",\"no\",\"none\",\"neither\",\"never\" or “nobody”\n",
    "\n",
    "    Step : 3\n",
    "        - Convert digit to \"_digit\"\n",
    "\n",
    "    Step : 4\n",
    "        - Remove @USER\n",
    "\n",
    "    Step : 5\n",
    "        - Remove unicode issue\n",
    "\n",
    "    Step 6: Lemmanize\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    text = pd.DataFrame(list(db.messages.find(query)))\n",
    "\n",
    "    # Count stock\n",
    "\n",
    "    text[\"count_stock\"] = text[\"symbols\"].apply(lambda x: len(x))\n",
    "\n",
    "    # Extract single count\n",
    "\n",
    "    text = text[text[\"count_stock\"].isin([1])]\n",
    "\n",
    "    # text = df.copy()\n",
    "\n",
    "    # take negation into account\n",
    "    text[\"body_transform\"] = text[\"body\"].replace(\n",
    "        regex={\n",
    "            r\"\\bnothing\\b\": \"nothing_negword\",\n",
    "            r\"\\bno\\b\": \"no_negword\",\n",
    "            r\"\\bnone\\b\": \"none_negword\",\n",
    "            r\"\\bneither\\b\": \"neither_negword\",\n",
    "            r\"\\bnever\\b\": \"never_negword\",\n",
    "            r\"\\bnobody\\b\": \"nobody_negword\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Convert digit to \"_digit\"\n",
    "    # Remove @USER\n",
    "    # Remove unicode issue\n",
    "    # Remove ticker\n",
    "    # Remove all the special characters\n",
    "    # remove all single characters\n",
    "    # Remove Ya\n",
    "    # Remove bitcoin\n",
    "    # remove btc\n",
    "\n",
    "    text[\"body_transform\"] = text[\"body_transform\"].replace(\n",
    "        regex={\n",
    "            r\"\\d+\": \"isDigit\",\n",
    "            r\"([@?])(\\w+)\\b\": \"user\",\n",
    "            r\"\\b&#\\b\": \" \",\n",
    "            r\"[$][A-Za-z][\\S]*\": \"\",\n",
    "            r\"\\W\": \" \",\n",
    "            r\"\\s+[a-zA-Z]\\s+\": \" \",\n",
    "            r\"\\^[a-zA-Z]\\s+\": \" \",\n",
    "            r\"\\s+\": \" \",\n",
    "            r\"^b\\s+\": \"\",\n",
    "            r\"\\bya\\b\": \"\",\n",
    "            r\"\\bbitcoin\\b\": \"\",\n",
    "            r\"\\bBitcoin\\b\": \"\",\n",
    "            r\"\\bbtc\\b\": \"\",\n",
    "\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Lower\n",
    "\n",
    "    text[\"body_transform\"] = text[\"body_transform\"].str.lower()\n",
    "\n",
    "    # Remove stop words\n",
    "\n",
    "    stop = stopwords.words('english')\n",
    "\n",
    "    text[\"body_transform\"] = text[\"body_transform\"].apply(\n",
    "        lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "    # Lemmatize\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "\n",
    "    text[\"body_transform\"] = text[\"body_transform\"].apply(\n",
    "        lambda x: \" \".join([lemmatizer.lemmatize(w)\n",
    "                            for w in w_tokenizer.tokenize(x)])\n",
    "    )\n",
    "\n",
    "    # Split the dataset\n",
    "\n",
    "    X_ = text[\"body_transform\"]\n",
    "    y_ = text[\"sentiment_\"]\n",
    "\n",
    "    count_ = text.groupby(\"sentiment\")[\"sentiment\"].count()\n",
    "\n",
    "    print(\"The shape of the data is {}, and {}\".format(text.shape, count_))\n",
    "\n",
    "    if to_train:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_, y_, test_size=0.1, random_state=0\n",
    "        )\n",
    "\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    else:\n",
    "\n",
    "        return X_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Pipeline step\n",
    "\n",
    "This step includes:\n",
    "\n",
    "- Build the Vectorization\n",
    "- Construct the Naive classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Example of stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "stopwords.words('english')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(max_features=1500,\n",
    "                             min_df=10,\n",
    "                             max_df=0.7)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    " ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Create the first transformation of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "query ={\n",
    "    \"sentiment\":{ \"$ne\": \"Neutral\" }\n",
    "}\n",
    "\n",
    "X_train, X_test, y_train, y_test = metatransformation(query = query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Quick stat descriptive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "Word_tokenize = X_train.apply(word_tokenize) \n",
    "### Need to flatten the list\n",
    "flattened_list = [y for x in Word_tokenize.tolist() for y in x]\n",
    "fdist = FreqDist(flattened_list)\n",
    "fdist.plot(30,cumulative=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "y_train.reset_index().groupby('sentiment_')['sentiment_'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def plot_keyword_sentiment(df, nbKeyword= 10):\n",
    "    \"\"\"\n",
    "    Plot the distribution of sentiments by keyword\n",
    "    \"\"\"\n",
    "    \n",
    "    df_fdist = pd.DataFrame.from_dict(df, orient='index')\n",
    "    df_fdist.columns = ['Frequency']\n",
    "    df_fdist.index.name = 'Term'\n",
    "    df_fdist =df_fdist.sort_values(by = 'Frequency', ascending = False)\n",
    "    \n",
    "    ### \n",
    "    \n",
    "    df_top_sent = pd.DataFrame()\n",
    "    for key in df_fdist.head(nbKeyword).index:\n",
    "\n",
    "        count_sentiment = (\n",
    "            pd.concat([X_train[X_train.str.contains(key)],\n",
    "                             y_train], axis = 1, join = 'inner')\n",
    "            .groupby('sentiment_')['body_transform']\n",
    "            .count()\n",
    "            .reset_index()\n",
    "        )\n",
    "        count_sentiment['keyword'] = key\n",
    "        df_top_sent = df_top_sent.append(count_sentiment)\n",
    "    df_top_sent = df_top_sent.pivot(index='keyword',\n",
    "                  columns='sentiment_',\n",
    "                  values='body_transform')\n",
    "    df_top_sent['sum'] = df_top_sent.apply(lambda x: x.sum(), axis = 1)\n",
    "    df_top_sent.sort_values(by = 'sum').drop(columns = 'sum').plot.barh(stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "plot_keyword_sentiment(df = fdist, nbKeyword= 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Bigrams\n",
    "\n",
    "Definitelly needs to clean more the corpus.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "bgs = nltk.ngrams(flattened_list, 2)\n",
    "\n",
    "fdist = nltk.FreqDist(bgs)\n",
    "\n",
    "fdist.plot(30,cumulative=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "text_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "y_pred = text_clf.predict(X_train)\n",
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "predicted = text_clf.predict(X_test)\n",
    "\n",
    "print(metrics.classification_report(y_test,\n",
    "                                    predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Predict out of sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "query ={\n",
    "    \"sentiment\":\"Neutral\" \n",
    "}\n",
    "X_predict = metatransformation(query = query,\n",
    "                               to_train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "predicted = text_clf.predict(X_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "pd.concat([pd.Series(X_predict, name = 'body').reset_index(),\n",
    "          pd.Series(predicted, name = 'predict')], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## Get Bitcoins Data\n",
    "\n",
    "Extracted from [Quandl](https://www.quandl.com/data/BCHAIN/MKPRU-Bitcoin-Market-Price-USD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import quandl\n",
    "quandl.ApiConfig.api_key = \"gs_J3domJb8kT6WjLz9s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "bitcoin = quandl.get(\"BCHAIN/MKPRU\")\n",
    "bitcoin['returns'] = bitcoin.pct_change(1)\n",
    "bitcoin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "bitcoin['Value'].plot(title='Values of Bitcoins')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "bitcoin['returns'].dropna().plot(title='Returns of Bitcoins')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## Daily aggregated sentiment\n",
    "\n",
    "Compute the daily average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "query = {\"sentiment\": {\"$ne\": \"Neutral\"}}\n",
    "text = pd.DataFrame(list(db.messages.find(query)))\n",
    "text[\"created_at\"] = pd.to_datetime(text[\"created_at\"], infer_datetime_format=True)\n",
    "text = (text\n",
    "        .set_index(\"created_at\")\n",
    "        .drop(columns=\"id\")\n",
    "        .resample(\"D\")\n",
    "        .mean()\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "timeseries = pd.concat([text, bitcoin], axis = 1, join=\"inner\")\n",
    "\n",
    "timeseries.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "axw = timeseries[['sentiment_', 'returns']].plot(secondary_y = 'returns',\n",
    "                      title='Relationship betxeen sentiments and Bitcoins return')\n",
    "figw = axw.get_figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## Granger test\n",
    "\n",
    "Test the Granger Causality between sentiment on social media and stock returns\n",
    "\n",
    "### How does Granger causality test work?\n",
    "\n",
    "It is based on the idea that if X causes Y, then the forecast of Y based on previous values of Y AND the previous values of X should outperform the forecast of Y based on previous values of Y alone.\n",
    "\n",
    "According to Statsmodels \n",
    "\n",
    "The Null hypothesis for `grangercausalitytests` is that the time series in the second column, x2, does NOT Granger cause the time series in the first column, x1. Grange causality means that past values of x2 have a statistically significant effect on the current value of x1, taking past values of x1 into account as regressors. We reject the null hypothesis that x2 does not Granger cause x1 if the pvalues are below a desired size of the test.\n",
    "\n",
    "The null hypothesis for all four test is that the coefficients corresponding to past values of the second time series are zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import grangercausalitytests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "grangercausalitytests(timeseries[['returns', 'sentiment_']], maxlag=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## Regress\n",
    "\n",
    "$$r_{i, t}=\\alpha+\\beta_{1} \\Delta s_{1, t}+\\beta_{2} \\Delta s_{i, t-1}+\\epsilon_{t}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "timeseries['sentiment_lag'] = timeseries['sentiment_'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "timeseries['L_s1'] = timeseries['sentiment_'].pct_change(1)\n",
    "timeseries['L_s2'] = timeseries['sentiment_lag'].pct_change(1)\n",
    "timeseries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "mod1 = smf.ols(formula='returns ~ L_s1 + L_s2', \n",
    "               data=timeseries).fit()\n",
    "mod1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## test Lexicon L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "L1 = pd.read_csv('http://www.thomas-renault.com/l1_lexicon.csv', sep = \";\")\n",
    "L1.sort_values(by = 'keyword').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "Word_tokenize = X_train.apply(word_tokenize) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "# Appendix: Details steps & analytics\n",
    "\n",
    "## Text Analysis Operations using NLTK\n",
    "\n",
    "We use the full set with Bullish and Bearish \n",
    "\n",
    "### Tokenise pandas series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "### TF-IDF:  Our approach\n",
    "\n",
    "As explained in the previous post, the tf-idf vectorization of a corpus of text documents assigns each word in a document a number that is proportional to its frequency in the document and inversely proportional to the number of documents in which it occurs\n",
    "\n",
    "TF: Term Frequency, which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization:\n",
    "\n",
    "TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).\n",
    "\n",
    "IDF: Inverse Document Frequency, which measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following:\n",
    "\n",
    "IDF(t) = log_e(Total number of documents / Number of documents with term t in it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Compute the IDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Compute the TFIDF score\n",
    "\n",
    "The higher the TF*IDF score (weight), the rarer the term and vice versa."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
